{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "    Sample code to reproduce our results for the Bayesian neural network example.\n",
    "    Our settings are almost the same as Hernandez-Lobato and Adams (ICML15) https://jmhldotorg.files.wordpress.com/2015/05/pbp-icml2015.pdf\n",
    "    Our implementation is also based on their Python code.\n",
    "    \n",
    "    p(y | W, X, \\gamma) = \\prod_i^N  N(y_i | f(x_i; W), \\gamma^{-1})\n",
    "    p(W | \\lambda) = \\prod_i N(w_i | 0, \\lambda^{-1})\n",
    "    p(\\gamma) = Gamma(\\gamma | a0, b0)\n",
    "    p(\\lambda) = Gamma(\\lambda | a0, b0)\n",
    "    \n",
    "    The posterior distribution is as follows:\n",
    "    p(W, \\gamma, \\lambda) = p(y | W, X, \\gamma) p(W | \\lanmbda) p(\\gamma) p(\\lambda) \n",
    "    To avoid negative values of \\gamma and \\lambda, we update loggamma and loglambda instead.\n",
    "    \n",
    "    Copyright (c) 2016,  Qiang Liu & Dilin Wang\n",
    "    All rights reserved.\n",
    "'''\n",
    "\n",
    "\n",
    "class svgd_bayesnn:\n",
    "\n",
    "    '''\n",
    "        We define a one-hidden-layer-neural-network specifically. We leave extension of deep neural network as our future work.\n",
    "        \n",
    "        Input\n",
    "            -- X_train: training dataset, features\n",
    "            -- y_train: training labels\n",
    "            -- batch_size: sub-sampling batch size\n",
    "            -- max_iter: maximum iterations for the training procedure\n",
    "            -- M: number of particles are used to fit the posterior distribution\n",
    "            -- n_hidden: number of hidden units\n",
    "            -- a0, b0: hyper-parameters of Gamma distribution\n",
    "            -- master_stepsize, auto_corr: parameters of adgrad\n",
    "    '''\n",
    "    def __init__(self, X_train, y_train,  batch_size = 100, max_iter = 100, M = 100, n_hidden = 50, a0 = 1, b0 = 0.1, master_stepsize = 1e-3, auto_corr = 0.9,beta=-0.5):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.d = X_train.shape[1]   # number of data, dimension \n",
    "        self.M = M\n",
    "        self.beta=beta\n",
    "        self.auto_corr=auto_corr\n",
    "\n",
    "        self.master_stepsize=master_stepsize\n",
    "        num_vars = self.d * n_hidden + n_hidden * 2 + 3  # w1: d*n_hidden; b1: n_hidden; w2 = n_hidden; b2 = 1; 2 variances\n",
    "        self.theta = np.zeros([self.M, num_vars])  # particles, will be initialized later\n",
    "        \n",
    "        '''\n",
    "            We keep the last 10% (maximum 500) of training data points for model developing\n",
    "        '''\n",
    "        size_dev = min(int(np.round(0.1 * X_train.shape[0])), 500)\n",
    "        X_dev, y_dev = X_train[-size_dev:], y_train[-size_dev:]\n",
    "        X_train, y_train = X_train[:-size_dev], y_train[:-size_dev]\n",
    "\n",
    "        '''\n",
    "            The data sets are normalized so that the input features and the targets have zero mean and unit variance\n",
    "        '''\n",
    "        self.std_X_train = np.std(X_train, 0)\n",
    "        self.std_X_train[ self.std_X_train == 0 ] = 1\n",
    "        self.mean_X_train = np.mean(X_train, 0)\n",
    "                \n",
    "        self.mean_y_train = np.mean(y_train)\n",
    "        self.std_y_train = np.std(y_train)\n",
    "        \n",
    "        '''\n",
    "            Theano symbolic variables\n",
    "            Define the neural network here\n",
    "        '''\n",
    "        X = T.matrix('X') # Feature matrix\n",
    "        y = T.vector('y') # labels\n",
    "        \n",
    "        w_1 = T.matrix('w_1') # weights between input layer and hidden layer\n",
    "        b_1 = T.vector('b_1') # bias vector of hidden layer\n",
    "        w_2 = T.vector('w_2') # weights between hidden layer and output layer\n",
    "        b_2 = T.scalar('b_2') # bias of output\n",
    "        \n",
    "        N = T.scalar('N') # number of observations\n",
    "        \n",
    "        log_gamma = T.scalar('log_gamma')   # variances related parameters\n",
    "        log_lambda = T.scalar('log_lambda')\n",
    "        \n",
    "        ###\n",
    "        prediction = T.dot(T.nnet.relu(T.dot(X, w_1)+b_1), w_2) + b_2\n",
    "        \n",
    "        ''' define the log posterior distribution '''\n",
    "        log_lik_data = -0.5 * X.shape[0] * (T.log(2*np.pi) - log_gamma) - (T.exp(log_gamma)/2) * T.sum(T.power(prediction - y, 2))\n",
    "        log_prior_data = (a0 - 1) * log_gamma - b0 * T.exp(log_gamma) + log_gamma\n",
    "        log_prior_w = -0.5 * (num_vars-2) * (T.log(2*np.pi)-log_lambda) - (T.exp(log_lambda)/2)*((w_1**2).sum() + (w_2**2).sum() + (b_1**2).sum() + b_2**2)  \\\n",
    "                       + (a0-1) * log_lambda - b0 * T.exp(log_lambda) + log_lambda\n",
    "        \n",
    "        # sub-sampling mini-batches of data, where (X, y) is the batch data, and N is the number of whole observations\n",
    "        log_posterior = (log_lik_data * N / X.shape[0] + log_prior_data + log_prior_w)\n",
    "        dw_1, db_1, dw_2, db_2, d_log_gamma, d_log_lambda = T.grad(log_posterior, [w_1, b_1, w_2, b_2, log_gamma, log_lambda])\n",
    "        \n",
    "        # automatic gradient\n",
    "        self.logp_gradient = theano.function(\n",
    "             inputs = [X, y, w_1, b_1, w_2, b_2, log_gamma, log_lambda, N],\n",
    "             outputs = [dw_1, db_1, dw_2, db_2, d_log_gamma, d_log_lambda]\n",
    "        )\n",
    "        \n",
    "        # prediction function\n",
    "        self.nn_predict = theano.function(inputs = [X, w_1, b_1, w_2, b_2], outputs = prediction)\n",
    "        \n",
    "        '''\n",
    "            Training with SVGD(beta=0) and Beta SVGD\n",
    "        '''\n",
    "        # normalization\n",
    "        X_train, y_train = self.normalization(X_train, y_train)\n",
    "        N0 = X_train.shape[0]  # number of observations\n",
    "        \n",
    "        ''' initializing all particles '''\n",
    "        #np.random.seed(300)\n",
    "        for i in range(self.M):\n",
    "            w1, b1, w2, b2, loggamma, loglambda = self.init_weights(a0, b0)\n",
    "            # use better initialization for gamma\n",
    "            ridx = np.random.choice(range(X_train.shape[0]), \\\n",
    "                                           np.min([X_train.shape[0], 1000]), replace = False)\n",
    "            y_hat = self.nn_predict(X_train[ridx,:], w1, b1, w2, b2)\n",
    "            loggamma = -np.log(np.mean(np.power(y_hat - y_train[ridx], 2)))\n",
    "            self.theta[i,:] = self.pack_weights(w1, b1, w2, b2, loggamma, loglambda)\n",
    "\n",
    "\n",
    "\n",
    "        '''self variables'''    \n",
    "        self.X_train = X_train\n",
    "        self.y_train=y_train\n",
    "        self.N0=N0\n",
    "        self.num_vars=num_vars\n",
    "        self.max_iter=max_iter\n",
    "        self.batch_size=batch_size\n",
    "        self.X_dev=X_dev\n",
    "        self.y_dev=y_dev\n",
    "        self.grad_theta = np.zeros([self.M, self.num_vars])  # gradient \n",
    "        omega = 1/self.M*np.ones(self.M)\n",
    "        d_kernal_xj_xi = np.zeros([self.M,self.M])\n",
    "\n",
    "        bandwidth = np.sqrt(self.theta.shape(1))\n",
    "        for iter in range(self.max_iter):\n",
    "            # sub-sampling\n",
    "            batch = [ i % self.N0 for i in range(iter * self.batch_size, (iter + 1) * self.batch_size) ]\n",
    "            for i in range(self.M):\n",
    "                w1, b1, w2, b2, loggamma, loglambda = self.unpack_weights(self.theta[i,:])\n",
    "                dw1, db1, dw2, db2, dloggamma, dloglambda = self.logp_gradient(self.X_train[batch,:], self.y_train[batch], w1, b1, w2, b2, loggamma, loglambda, self.N0)\n",
    "                self.grad_theta[i,:] = self.pack_weights(dw1, db1, dw2, db2, dloggamma, dloglambda)\n",
    "                \n",
    "            # calculating the kernel matrix\n",
    "            if self.beta==0:\n",
    "                kxy, dxkxy = self.svgd_kernel(h=bandwidth)  \n",
    "                self.grad_theta = (np.matmul(kxy, self.grad_theta) + dxkxy) / self.M   \n",
    "\n",
    "            else:\n",
    "            \n",
    "                kxy, dxkxy, ddkxy, hh = self.betasvgd_kernel(h=bandwidth) \n",
    "    \n",
    "                for i_index in range(self.M):\n",
    "                    for j_index in range(self.M):\n",
    "                        d_kernal_xj_xi[i_index][j_index] = np.matmul(self.theta[i_index]-self.theta[j_index],(self.grad_theta[i_index]-self.grad_theta[j_index]).transpose())\n",
    "                d_kernal_xj_xi = np.multiply((d_kernal_xj_xi)*(2/hh),kxy)\n",
    "\n",
    "                SteinMatrix = (np.multiply(kxy,np.matmul(self.grad_theta,self.grad_theta.transpose()))+ddkxy+d_kernal_xj_xi) /10**4  #rescale the Stein Matrix by a factor 10**4\n",
    "            \n",
    "                omega = self.mirror_descent(SteinMatrix,50,0.5,omega,0.2) \n",
    "\n",
    "                self.grad_theta = (np.matmul(kxy, self.grad_theta) + dxkxy) / self.M   \n",
    "                self.grad_theta = np.matmul(np.diag(np.maximum(self.M*omega,0.25)**(self.beta)),self.grad_theta)\n",
    "\n",
    "            self.theta = self.theta + self.master_stepsize * self.grad_theta \n",
    "\n",
    "\n",
    "        '''\n",
    "            Model selection by using a development set\n",
    "\n",
    "        '''\n",
    "\n",
    "        X_dev = self.normalization(self.X_dev) \n",
    "        for i in range(self.M):\n",
    "            w1, b1, w2, b2, loggamma, loglambda = self.unpack_weights(self.theta[i, :])\n",
    "            pred_y_dev = self.nn_predict(X_dev, w1, b1, w2, b2) * self.std_y_train + self.mean_y_train\n",
    "            # likelihood\n",
    "            def f_log_lik(loggamma): return np.sum(  np.log(np.sqrt(np.exp(loggamma)) /np.sqrt(2*np.pi) * np.exp( -1 * (np.power(pred_y_dev - self.y_dev, 2) / 2) * np.exp(loggamma) )) )\n",
    "            # The higher probability is better    \n",
    "            lik1 = f_log_lik(loggamma)\n",
    "            # one heuristic setting\n",
    "            loggamma = -np.log(np.mean(np.power(pred_y_dev -self.y_dev, 2)))\n",
    "            lik2 = f_log_lik(loggamma)\n",
    "            if lik2 > lik1:\n",
    "                self.theta[i,-2] = loggamma  # update loggamma\n",
    "\n",
    "\n",
    "    def normalization(self, X, y = None):\n",
    "        X = (X - np.full(X.shape, self.mean_X_train)) / \\\n",
    "            np.full(X.shape, self.std_X_train)\n",
    "            \n",
    "        if y is not None:\n",
    "            y = (y - self.mean_y_train) / self.std_y_train\n",
    "            return (X, y)  \n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    '''\n",
    "        Initialize all particles\n",
    "    '''\n",
    "    def init_weights(self, a0, b0):\n",
    "        w1 = 1.0 / np.sqrt(self.d + 1) * np.random.randn(self.d, self.n_hidden)\n",
    "        b1 = np.zeros((self.n_hidden,))\n",
    "        w2 = 1.0 / np.sqrt(self.n_hidden + 1) * np.random.randn(self.n_hidden)\n",
    "        b2 = 0.\n",
    "        loggamma = np.log(np.random.gamma(a0, b0))\n",
    "        loglambda = np.log(np.random.gamma(a0, b0))\n",
    "        return (w1, b1, w2, b2, loggamma, loglambda)\n",
    "    \n",
    "    '''\n",
    "        Calculate kernel matrix and its gradient: K, \\nabla_x k\n",
    "    ''' \n",
    "    def svgd_kernel(self, h = -1):\n",
    "        sq_dist = pdist(self.theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = np.sqrt(0.5 * h / np.log(self.theta.shape[0]+1))\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        \n",
    "        Kxy = np.exp( -pairwise_dists / h**2 / 2)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, self.theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(self.theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(self.theta[:,i],sumkxy)\n",
    "        dxkxy = dxkxy / (h**2)\n",
    "        return (Kxy, dxkxy)\n",
    "\n",
    "    def betasvgd_kernel(self, h = -1):\n",
    "        d = self.theta.shape[1]\n",
    "        sq_dist = pdist(self.theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = np.sqrt(0.5 * h / np.log(self.theta.shape[0]+1))\n",
    "\n",
    "        Kxy = np.exp( -pairwise_dists / h**2 / 2) # the bandwith for the reproducing kernal here is 2*h**2\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, self.theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(self.theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(self.theta[:,i],sumkxy)\n",
    "        dxkxy = dxkxy / (h**2)\n",
    "        \n",
    "        ddkxy=np.zeros([self.theta.shape[0],self.theta.shape[0]])\n",
    "        ddkxy= np.multiply(Kxy,d/h**2-pairwise_dists/h**4)\n",
    "        return (Kxy, dxkxy, ddkxy, 2*h**2)\n",
    "    \n",
    "    def mirror_descent(self,A,iter,stepsize,omega,alpha):\n",
    "        omega_m = omega\n",
    "        n = omega.shape[0]\n",
    "\n",
    "        for j in range(iter):\n",
    "            omega_m = alpha*omega_m + np.matmul(A,omega)\n",
    "            for i in range(n):\n",
    "                omega[i]=omega[i]*np.exp(-stepsize*omega_m[i])\n",
    "            omega = (1/omega.sum())*omega \n",
    "\n",
    "        return omega \n",
    "    \n",
    "    \n",
    "    '''\n",
    "        Pack all parameters in our model\n",
    "    '''    \n",
    "    def pack_weights(self, w1, b1, w2, b2, loggamma, loglambda):\n",
    "        params = np.concatenate([w1.flatten(), b1, w2, [b2], [loggamma],[loglambda]])\n",
    "        return params\n",
    "    \n",
    "    '''\n",
    "        Unpack all parameters in our model\n",
    "    '''\n",
    "    def unpack_weights(self, z):\n",
    "        w = z\n",
    "        w1 = np.reshape(w[:self.d*self.n_hidden], [self.d, self.n_hidden])\n",
    "        b1 = w[self.d*self.n_hidden:(self.d+1)*self.n_hidden]\n",
    "    \n",
    "        w = w[(self.d+1)*self.n_hidden:]\n",
    "        w2, b2 = w[:self.n_hidden], w[-3] \n",
    "        \n",
    "        # the last two parameters are log variance\n",
    "        loggamma, loglambda= w[-2], w[-1]\n",
    "        \n",
    "        return (w1, b1, w2, b2, loggamma, loglambda)\n",
    "\n",
    "    \n",
    "    '''\n",
    "        Evaluating testing rmse and log-likelihood, which is the same as in PBP \n",
    "        Input:\n",
    "            -- X_test: unnormalized testing feature set\n",
    "            -- y_test: unnormalized testing labels\n",
    "    '''\n",
    "    def evaluation(self, X_test, y_test):\n",
    "        # normalization\n",
    "        X_test = self.normalization(X_test)\n",
    "        \n",
    "        # average over the output\n",
    "        pred_y_test = np.zeros([self.M, len(y_test)])\n",
    "        prob = np.zeros([self.M, len(y_test)])\n",
    "        \n",
    "        '''\n",
    "            Since we have M particles, we use a Bayesian view to calculate rmse and log-likelihood\n",
    "        '''\n",
    "        for i in range(self.M):\n",
    "            w1, b1, w2, b2, loggamma, loglambda = self.unpack_weights(self.theta[i, :])\n",
    "            pred_y_test[i, :] = self.nn_predict(X_test, w1, b1, w2, b2) * self.std_y_train + self.mean_y_train\n",
    "            prob[i, :] = np.sqrt(np.exp(loggamma)) /np.sqrt(2*np.pi) * np.exp( -1 * (np.power(pred_y_test[i, :] - y_test, 2) / 2) * np.exp(loggamma) )\n",
    "        pred = np.mean(pred_y_test, axis=0)\n",
    "        \n",
    "        # evaluation\n",
    "        svgd_rmse = np.sqrt(np.mean((pred - y_test)**2))\n",
    "        svgd_ll = np.mean(np.log(np.mean(prob, axis = 0)))\n",
    "        \n",
    "        return (svgd_rmse, svgd_ll)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=1 #we set it as 1 in the experiments\n",
    "testn=1\n",
    "betan =-0.5\n",
    "batch_size, n_hidden, max_iter,master_stepsize= 100, 50, 1000,1e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seeds)\n",
    "random.seed(seeds)\n",
    "''' load data file '''\n",
    "#data = np.loadtxt('boston_housing')\n",
    "data = np.loadtxt('seeds.txt')\n",
    "#data = np.loadtxt('yacht.txt')\n",
    "# Please make sure that the last column is the label and the other columns are features\n",
    "X_input = data[ :, range(data.shape[ 1 ] - 1) ]\n",
    "y_input = data[ :, data.shape[ 1 ] - 1 ]\n",
    "\n",
    "''' build the training and testing data set'''\n",
    "train_ratio = 0.9 # We create the train and test sets with 90% and 10% of the data\n",
    "permutation = np.arange(X_input.shape[0])\n",
    "random.shuffle(permutation) \n",
    "\n",
    "size_train = int(np.round(X_input.shape[ 0 ] * train_ratio))\n",
    "index_train = permutation[ 0 : size_train]\n",
    "index_test = permutation[ size_train : ]\n",
    "\n",
    "X_train, y_train = X_input[ index_train, : ], y_input[ index_train ]\n",
    "X_test, y_test = X_input[ index_test, : ], y_input[ index_test ]\n",
    "\n",
    "\n",
    "test_out1=np.zeros([10,2])\n",
    "for i in range(testn):\n",
    "    svgd = svgd_bayesnn(X_train, y_train, batch_size = batch_size, n_hidden = n_hidden, max_iter = max_iter,master_stepsize=master_stepsize,beta=0)\n",
    "    test_out1[i]=svgd.evaluation(X_test,y_test)\n",
    "print(test_out1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seeds)\n",
    "random.seed(seeds)\n",
    "''' load data file '''\n",
    "#data = np.loadtxt('boston_housing')\n",
    "data = np.loadtxt('seeds.txt')\n",
    "#data = np.loadtxt('yacht.txt')\n",
    "# Please make sure that the last column is the label and the other columns are features\n",
    "X_input = data[ :, range(data.shape[ 1 ] - 1) ]\n",
    "y_input = data[ :, data.shape[ 1 ] - 1 ]\n",
    "\n",
    "''' build the training and testing data set'''\n",
    "train_ratio = 0.9 # We create the train and test sets with 90% and 10% of the data\n",
    "permutation = np.arange(X_input.shape[0])\n",
    "random.shuffle(permutation) \n",
    "\n",
    "size_train = int(np.round(X_input.shape[ 0 ] * train_ratio))\n",
    "index_train = permutation[ 0 : size_train]\n",
    "index_test = permutation[ size_train : ]\n",
    "\n",
    "X_train, y_train = X_input[ index_train, : ], y_input[ index_train ]\n",
    "X_test, y_test = X_input[ index_test, : ], y_input[ index_test ]\n",
    "\n",
    "\n",
    "test_out2=np.zeros([10,2])\n",
    "for i in range(testn):\n",
    "    svgd = svgd_bayesnn(X_train, y_train, batch_size = batch_size, n_hidden = n_hidden, max_iter = max_iter,master_stepsize=master_stepsize,beta=betan)\n",
    "    test_out2[i]=svgd.evaluation(X_test,y_test)\n",
    "print(test_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seeds)\n",
    "random.seed(seeds)\n",
    "''' load data file '''\n",
    "data = np.loadtxt('boston_housing')\n",
    "#data = np.loadtxt('seeds.txt')\n",
    "#data = np.loadtxt('yacht.txt')\n",
    "# Please make sure that the last column is the label and the other columns are features\n",
    "X_input = data[ :, range(data.shape[ 1 ] - 1) ]\n",
    "y_input = data[ :, data.shape[ 1 ] - 1 ]\n",
    "\n",
    "''' build the training and testing data set'''\n",
    "train_ratio = 0.9 # We create the train and test sets with 90% and 10% of the data\n",
    "permutation = np.arange(X_input.shape[0])\n",
    "random.shuffle(permutation) \n",
    "\n",
    "size_train = int(np.round(X_input.shape[ 0 ] * train_ratio))\n",
    "index_train = permutation[ 0 : size_train]\n",
    "index_test = permutation[ size_train : ]\n",
    "\n",
    "X_train, y_train = X_input[ index_train, : ], y_input[ index_train ]\n",
    "X_test, y_test = X_input[ index_test, : ], y_input[ index_test ]\n",
    "\n",
    "\n",
    "test_out3=np.zeros([10,2])\n",
    "for i in range(testn):\n",
    "    svgd = svgd_bayesnn(X_train, y_train, batch_size = batch_size, n_hidden = n_hidden, max_iter = max_iter,master_stepsize=master_stepsize,beta=0)\n",
    "    test_out3[i]=svgd.evaluation(X_test,y_test)\n",
    "print(test_out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seeds)\n",
    "random.seed(seeds)\n",
    "''' load data file '''\n",
    "data = np.loadtxt('boston_housing')\n",
    "#data = np.loadtxt('seeds.txt')\n",
    "#data = np.loadtxt('yacht.txt')\n",
    "# Please make sure that the last column is the label and the other columns are features\n",
    "X_input = data[ :, range(data.shape[ 1 ] - 1) ]\n",
    "y_input = data[ :, data.shape[ 1 ] - 1 ]\n",
    "\n",
    "''' build the training and testing data set'''\n",
    "train_ratio = 0.9 # We create the train and test sets with 90% and 10% of the data\n",
    "permutation = np.arange(X_input.shape[0])\n",
    "random.shuffle(permutation) \n",
    "\n",
    "size_train = int(np.round(X_input.shape[ 0 ] * train_ratio))\n",
    "index_train = permutation[ 0 : size_train]\n",
    "index_test = permutation[ size_train : ]\n",
    "\n",
    "X_train, y_train = X_input[ index_train, : ], y_input[ index_train ]\n",
    "X_test, y_test = X_input[ index_test, : ], y_input[ index_test ]\n",
    "\n",
    "\n",
    "test_out4=np.zeros([10,2])\n",
    "for i in range(testn):\n",
    "    svgd = svgd_bayesnn(X_train, y_train, batch_size = batch_size, n_hidden = n_hidden, max_iter = max_iter,master_stepsize=master_stepsize,beta=betan)\n",
    "    test_out4[i]=svgd.evaluation(X_test,y_test)\n",
    "print(test_out4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seeds)\n",
    "random.seed(seeds)\n",
    "''' load data file '''\n",
    "#data = np.loadtxt('boston_housing')\n",
    "#data = np.loadtxt('seeds.txt')\n",
    "data = np.loadtxt('yacht.txt')\n",
    "# Please make sure that the last column is the label and the other columns are features\n",
    "X_input = data[ :, range(data.shape[ 1 ] - 1) ]\n",
    "y_input = data[ :, data.shape[ 1 ] - 1 ]\n",
    "\n",
    "''' build the training and testing data set'''\n",
    "train_ratio = 0.9 # We create the train and test sets with 90% and 10% of the data\n",
    "permutation = np.arange(X_input.shape[0])\n",
    "random.shuffle(permutation) \n",
    "\n",
    "size_train = int(np.round(X_input.shape[ 0 ] * train_ratio))\n",
    "index_train = permutation[ 0 : size_train]\n",
    "index_test = permutation[ size_train : ]\n",
    "\n",
    "X_train, y_train = X_input[ index_train, : ], y_input[ index_train ]\n",
    "X_test, y_test = X_input[ index_test, : ], y_input[ index_test ]\n",
    "\n",
    "\n",
    "test_out5=np.zeros([10,2])\n",
    "for i in range(testn):\n",
    "    svgd = svgd_bayesnn(X_train, y_train, batch_size = batch_size, n_hidden = n_hidden, max_iter = max_iter,master_stepsize=master_stepsize,beta=0)\n",
    "    test_out5[i]=svgd.evaluation(X_test,y_test)\n",
    "print(test_out5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seeds)\n",
    "random.seed(seeds)\n",
    "''' load data file '''\n",
    "#data = np.loadtxt('boston_housing')\n",
    "#data = np.loadtxt('seeds.txt')\n",
    "data = np.loadtxt('yacht.txt')\n",
    "# Please make sure that the last column is the label and the other columns are features\n",
    "X_input = data[ :, range(data.shape[ 1 ] - 1) ]\n",
    "y_input = data[ :, data.shape[ 1 ] - 1 ]\n",
    "\n",
    "''' build the training and testing data set'''\n",
    "train_ratio = 0.9 # We create the train and test sets with 90% and 10% of the data\n",
    "permutation = np.arange(X_input.shape[0])\n",
    "random.shuffle(permutation) \n",
    "\n",
    "size_train = int(np.round(X_input.shape[ 0 ] * train_ratio))\n",
    "index_train = permutation[ 0 : size_train]\n",
    "index_test = permutation[ size_train : ]\n",
    "\n",
    "X_train, y_train = X_input[ index_train, : ], y_input[ index_train ]\n",
    "X_test, y_test = X_input[ index_test, : ], y_input[ index_test ]\n",
    "\n",
    "\n",
    "test_out6=np.zeros([10,2])\n",
    "for i in range(testn):\n",
    "    svgd = svgd_bayesnn(X_train, y_train, batch_size = batch_size, n_hidden = n_hidden, max_iter = max_iter,master_stepsize=master_stepsize,beta=betan)\n",
    "    test_out6[i]=svgd.evaluation(X_test,y_test)\n",
    "print(test_out6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
